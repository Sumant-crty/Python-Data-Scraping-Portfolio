{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOHy2nLjzymIOxyT3nvYEn2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sumant-crty/Python-Data-Scraping-Portfolio/blob/main/Newsbroadcast.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "x0UWCHeA44tD",
        "outputId": "156855a2-0625-42d7-adda-369f20fd763e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: newspaper3k in /usr/local/lib/python3.12/dist-packages (0.2.8)\n",
            "Collecting lxml_html_clean\n",
            "  Downloading lxml_html_clean-0.4.3-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.12/dist-packages (from newspaper3k) (4.13.5)\n",
            "Requirement already satisfied: Pillow>=3.3.0 in /usr/local/lib/python3.12/dist-packages (from newspaper3k) (11.3.0)\n",
            "Requirement already satisfied: PyYAML>=3.11 in /usr/local/lib/python3.12/dist-packages (from newspaper3k) (6.0.3)\n",
            "Requirement already satisfied: cssselect>=0.9.2 in /usr/local/lib/python3.12/dist-packages (from newspaper3k) (1.3.0)\n",
            "Requirement already satisfied: lxml>=3.6.0 in /usr/local/lib/python3.12/dist-packages (from newspaper3k) (6.0.2)\n",
            "Requirement already satisfied: nltk>=3.2.1 in /usr/local/lib/python3.12/dist-packages (from newspaper3k) (3.9.1)\n",
            "Requirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.12/dist-packages (from newspaper3k) (2.32.4)\n",
            "Requirement already satisfied: feedparser>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from newspaper3k) (6.0.12)\n",
            "Requirement already satisfied: tldextract>=2.0.1 in /usr/local/lib/python3.12/dist-packages (from newspaper3k) (5.3.0)\n",
            "Requirement already satisfied: feedfinder2>=0.0.4 in /usr/local/lib/python3.12/dist-packages (from newspaper3k) (0.0.4)\n",
            "Requirement already satisfied: jieba3k>=0.35.1 in /usr/local/lib/python3.12/dist-packages (from newspaper3k) (0.35.1)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from newspaper3k) (2.9.0.post0)\n",
            "Requirement already satisfied: tinysegmenter==0.3 in /usr/local/lib/python3.12/dist-packages (from newspaper3k) (0.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4>=4.4.1->newspaper3k) (2.8)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4>=4.4.1->newspaper3k) (4.15.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from feedfinder2>=0.0.4->newspaper3k) (1.17.0)\n",
            "Requirement already satisfied: sgmllib3k in /usr/local/lib/python3.12/dist-packages (from feedparser>=5.2.1->newspaper3k) (1.0.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk>=3.2.1->newspaper3k) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk>=3.2.1->newspaper3k) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk>=3.2.1->newspaper3k) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk>=3.2.1->newspaper3k) (4.67.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.10.0->newspaper3k) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.10.0->newspaper3k) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.10.0->newspaper3k) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.10.0->newspaper3k) (2025.11.12)\n",
            "Requirement already satisfied: requests-file>=1.4 in /usr/local/lib/python3.12/dist-packages (from tldextract>=2.0.1->newspaper3k) (3.0.1)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.12/dist-packages (from tldextract>=2.0.1->newspaper3k) (3.20.0)\n",
            "Downloading lxml_html_clean-0.4.3-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: lxml_html_clean\n",
            "Successfully installed lxml_html_clean-0.4.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<h1>Latest News Headlines - 16-12-2025</h1>\n",
              "<ul>\n",
              "  <li><a href=\"https://timesofindia.indiatimes.com/life-style/relationships\" target=\"_blank\">Relationship, Man Woman Relationship Advice, Parenting Advice & Dating Tips</a></li>\n",
              "  <li><a href=\"https://timesofindia.indiatimes.com/city/delhi\" target=\"_blank\">Delhi News:दिल्ली समाचार -Latest Delhi News Headlines & Live Updates from Delhi, Delhi Local News Updates, Breaking Delhi News</a></li>\n",
              "  <li><a href=\"https://timesofindia.indiatimes.com/astrology/zodiacs-astrology/the-last-days-of-december-are-especially-favorable-for-these-3-zodiac-signs/articleshow/125997336.cms\" target=\"_blank\">The Last Days of December Are Especially Favorable for These 3 Zodiac Signs</a></li>\n",
              "  <li><a href=\"https://timesofindia.indiatimes.com/city/jaipur\" target=\"_blank\">Jaipur News, Latest Jaipur News Headlines & Live Updates</a></li>\n",
              "  <li><a href=\"https://timesofindia.indiatimes.com/entertainment/malayalam/mollywood/top-20-best-malayalam-movies-of-2025\" target=\"_blank\">Best Malayalam Films 2025 - Times of India</a></li>\n",
              "  <li><a href=\"https://timesofindia.indiatimes.com/world/us/the-trump-doctrine-how-united-states-sees-india-in-a-china-first-world/articleshow/126004888.cms\" target=\"_blank\">The Trump doctrine: How United States sees India in a China-first world</a></li>\n",
              "  <li><a href=\"https://timesofindia.indiatimes.com/sports/cricket/news/fresh-challenge-hanuma-vihari-joins-tripura-after-leaving-andhra-pradesh/articleshow/123529151.cms\" target=\"_blank\">'Fresh challenge': Hanuma Vihari joins Tripura after leaving Andhra Pradesh</a></li>\n",
              "  <li><a href=\"https://timesofindia.indiatimes.com/city/mangalore\" target=\"_blank\">Mangaluru News: Top Stories, Latest Headlines & Live City Updates</a></li>\n",
              "  <li><a href=\"https://timesofindia.indiatimes.com/business/india-business/axis-bank-sees-7-5-gdp-growth-not-overly-concerned-about-the-rupee/articleshow/126003761.cms\" target=\"_blank\">Axis Bank sees 7.5% GDP growth, not overly concerned about the rupee</a></li>\n",
              "  <li><a href=\"https://timesofindia.indiatimes.com/toi-plus/technology/is-ai-a-bubble-or-not/articleshow/125984757.cms\" target=\"_blank\">Is AI A Bubble Or Not?</a></li>\n",
              "  <li><a href=\"https://www.thehindu.com/life-and-style/seven-pink-patrol-vehicles-in-coimbatore-to-reach-out-to-women-in-distress/article70320932.ece\" target=\"_blank\">A strong push for women's security: Seven pink patrol vehicles launched in the city to ensure women's safety and freedom</a></li>\n",
              "  <li><a href=\"https://www.thehindu.com/entertainment/reviews/\" target=\"_blank\">Movie Reviews and Latest Movie Ratings</a></li>\n",
              "  <li><a href=\"https://www.thehindu.com/podcast/\" target=\"_blank\">Latest Podcast News</a></li>\n",
              "  <li><a href=\"https://www.thehindu.com/education/\" target=\"_blank\">Education News Today</a></li>\n",
              "  <li><a href=\"https://www.thehindu.com/news/international/bangladesh-ic-tribunal-verdict-sheikh-hasina/article70289801.ece\" target=\"_blank\">Former Bangladesh PM Hasina sentenced to death</a></li>\n",
              "  <li><a href=\"https://www.thehindu.com/news/cities/\" target=\"_blank\">News from Major Indian Metropolitan Cities Today</a></li>\n",
              "  <li><a href=\"https://www.thehindu.com/premium/ebook/\" target=\"_blank\">The Hindu e-books - The Genetic Frontier -How gene editing and synthetic biology are redefining what it means to be human. Subscriber Exclusive Ebooks - Subscribe, Unlock & Read</a></li>\n",
              "  <li><a href=\"https://www.thehindu.com/sport/football/court-rules-psg-must-pay-more-than-70m-to-mbappe-in-dispute-over-unpaid-wages/article70403441.ece\" target=\"_blank\">Court rules PSG must pay more than $70M to Mbappé in dispute over unpaid wages</a></li>\n",
              "  <li><a href=\"https://www.thehindu.com/business/markets/\" target=\"_blank\">Stock Market Today: Sensex, Nifty, BSE, NSE Latest Updates</a></li>\n",
              "  <li><a href=\"https://www.thehindu.com/sci-tech/energy-and-environment/study-affirms-keralas-rich-butterfly-diversity/article70403285.ece\" target=\"_blank\">Study affirms Kerala’s rich butterfly diversity</a></li>\n",
              "  <li><a href=\"https://www.bhaskar.com/local/uttar-pradesh/mathura/news/mathura-yamuna-express-way-bus-car-fire-accident-crime-seen-136683617.html\" target=\"_blank\">मथुरा हादसा- अचानक बस में धमाके हुए, शीशा तोड़कर कूदे: युवती बोली- फायर ब्रिगेड टाइम पर आती तो जानें बच जातीं - Mathura News</a></li>\n",
              "  <li><a href=\"https://www.bhaskar.com/entertainment/bollywood/video/zaira-wasim-gets-angry-after-nitish-kumar-pulls-her-hijab-136684616.html?type=widget&index=3&vid=13\" target=\"_blank\">nan</a></li>\n",
              "  <li><a href=\"https://www.bhaskar.com/madhurima/\" target=\"_blank\">Women and Girls In India, Trends, Expert Analysis & How-Tos - Dainik Bhaskar</a></li>\n",
              "  <li><a href=\"https://www.bhaskar.com/bhaskar-khaas/khabar-hatke/\" target=\"_blank\">Bihar Election Funny News - Dainik Bhaskar</a></li>\n",
              "  <li><a href=\"https://www.bhaskar.com/local/uttar-pradesh/lucknow/\" target=\"_blank\">Latest Lucknow News (लखनऊ न्यूज़): पढ़ें 16 दिसम्बर के ताज़ा समाचार दैनिक भास्कर पर</a></li>\n",
              "  <li><a href=\"https://www.bhaskar.com/local/uttar-pradesh/lucknow/news/administration-alert-before-cold-lucknow-dm-vishakh-g-rain-basera-inspection-winter-arrangements-136679069.html\" target=\"_blank\">ठंड को लेकर लखनऊ प्रशासन अलर्ट: डीएम विशाख जी ने रैन बसेरों का किया निरीक्षण, व्यवस्थाएं संतोषजनक मिलीं - Lucknow News</a></li>\n",
              "  <li><a href=\"https://www.bhaskar.com/local/mp/ujjain/news/hanuman-chalisa-recitation-at-the-dargah-136676002.html\" target=\"_blank\">दरगाह पर गूंजा हनुमान चालीसा पाठ,VIDEO: उज्जैन में शिष्यों के साथ पहुंचे थे संत; कमेटी बोली- चादर चढ़ाने और कव्वाली की अनुमति दी थी - Ujjain News</a></li>\n",
              "  <li><a href=\"https://www.bhaskar.com/entertainment/bollywood/news/sunny-deol-gets-emotional-at-border-2-teaser-launch-136686238.html\" target=\"_blank\">बॉर्डर 2 के टीजर लॉन्च पर सनी देओल भावुक हुए: डायलॉग बोलने के बाद आंसू पोंछते दिखे, वरुण धवन ने एक्टर के पैर छू लिया आशीर्वाद</a></li>\n",
              "  <li><a href=\"https://www.bhaskar.com/local/uttar-pradesh/\" target=\"_blank\">Latest Uttar Pradesh News (उत्तरप्रदेश न्यूज़): पढ़ें 16 दिसम्बर के ताज़ा समाचार दैनिक भास्कर पर</a></li>\n",
              "  <li><a href=\"https://www.bhaskar.com/local/mp/rewa/news/story-of-ramvilas-das-vedanti-maharaj-death-rewa-136679124.html\" target=\"_blank\">बचपन में घर छोड़ा, पैदल अयोध्या पहुंचे, वहीं रमे: परिवार मनाने पहुंचा तो बोले- अब यहीं मेरा घर; राम मंदिर आंदोलन का चेहरा बने वेदांती महाराज - Rewa News</a></li>\n",
              "</ul>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install newspaper3k lxml_html_clean\n",
        "\n",
        "# --- Import Libraries ---\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from newspaper import Article\n",
        "import csv\n",
        "import time\n",
        "import pandas as pd # Import pandas for DataFrame operations\n",
        "from datetime import datetime # Import datetime for current date\n",
        "from IPython.display import HTML # Import HTML for displaying web page content\n",
        "\n",
        "# --- Configuration ---\n",
        "BASE_URL = \"https://timesofindia.indiatimes.com/\"\n",
        "OUTPUT_FILENAME = \"toi_headlines.csv\"\n",
        "OUTPUT_ALL_HEADLINES_FILENAME = \"all_newspaper_headlines.csv\"\n",
        "HTML_OUTPUT_FILENAME = \"headlines.html\" # New: HTML output filename\n",
        "\n",
        "# CRUCIAL: Realistic User-Agent is necessary\n",
        "HEADERS = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "}\n",
        "\n",
        "NEWSPAPER_CONFIGS = {\n",
        "    'Times of India': {'url': 'https://timesofindia.indiatimes.com/', 'language': 'en'},\n",
        "    'The Hindu': {'url': 'https://www.thehindu.com/', 'language': 'en'},\n",
        "    'Dainik Bhaskar': {'url': 'https://www.bhaskar.com/', 'language': 'hi'}\n",
        "}\n",
        "\n",
        "def get_article_links(url, base_url):\n",
        "    \"\"\"Downloads the page and returns a set of unique article links.\"\"\"\n",
        "    try:\n",
        "        response = requests.get(url, headers=HEADERS, timeout=15)\n",
        "        response.raise_for_status() # Raise HTTPError for bad responses (4xx or 5xx)\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        article_links = set()\n",
        "        excluded_keywords = [\n",
        "            'javascript:', '#', '.css', '.js', '.jpg', '.png', '.gif', '.pdf',\n",
        "            'videos', 'photos', 'gallery', 'e-paper', 'epaper', 'subscribe', 'newsletter',\n",
        "            'login', 'signin', 'register', 'terms-of-use', 'privacy-policy', 'contact-us', 'about-us',\n",
        "            'advertise', 'careers', 'sitemap', 'authors', 'topics', 'tags', 'archives'\n",
        "        ]\n",
        "\n",
        "        for a_tag in soup.find_all('a', href=True):\n",
        "            href = a_tag['href']\n",
        "            full_link = \"\"\n",
        "\n",
        "            # Construct full URL if relative\n",
        "            if href.startswith('http') and base_url.split('//')[1].split('/')[0] in href:\n",
        "                full_link = href\n",
        "            elif href.startswith('/') and not href.startswith('//'): # Relative URL\n",
        "                full_link = base_url.rstrip('/') + href\n",
        "\n",
        "            # Filter out obvious non-article links and duplicates\n",
        "            if full_link and not any(keyword in full_link for keyword in excluded_keywords):\n",
        "                if full_link != base_url and len(full_link) > len(base_url) + 5: # Avoid just base URL and very short links\n",
        "                    article_links.add(full_link)\n",
        "\n",
        "        return article_links # Moved outside the for loop\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"ERROR: Could not fetch {url}. Reason: {e}\")\n",
        "        return set()\n",
        "\n",
        "def scrape_article_data(links, language, limit=10):\n",
        "    \"\"\"Scrapes the title, date, and URL for each article link, up to a specified limit.\"\"\"\n",
        "    scraped_data = []\n",
        "    # Convert set to list and take only the first 'limit' items\n",
        "    links_to_process = list(links)[:limit]\n",
        "\n",
        "\n",
        "    for i, link in enumerate(links_to_process):\n",
        "\n",
        "\n",
        "        try:\n",
        "            # Use newspaper3k for intelligent article content extraction\n",
        "            article = Article(link, language=language)\n",
        "            article.download()\n",
        "            article.parse()\n",
        "\n",
        "            scraped_data.append({\n",
        "                \"Title\": article.title,\n",
        "                \"Publish_Date\": str(article.publish_date), # Convert datetime object to string\n",
        "                \"URL\": link\n",
        "            })\n",
        "\n",
        "            # Introduce a short, polite delay between requests\n",
        "            time.sleep(1)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"WARNING: Failed to process article {link}. Skipping. Error: {e}\")\n",
        "            time.sleep(1) # Still pause if an error occurs\n",
        "            continue\n",
        "\n",
        "    return scraped_data\n",
        "\n",
        "def save_to_csv(data, filename):\n",
        "    \"\"\"Saves the list of dictionaries to a CSV file.\"\"\"\n",
        "    if not data:\n",
        "        print(\"No data to save.\")\n",
        "        return\n",
        "\n",
        "    # Use the keys of the first dictionary as the field names (headers)\n",
        "    fieldnames = data[0].keys()\n",
        "\n",
        "    with open(filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "\n",
        "        writer.writeheader() # Write the header row\n",
        "        writer.writerows(data) # Write all the data rows\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# --- Main Execution Block ---\n",
        "if __name__ == \"__main__\":\n",
        "    all_scraped_data = []\n",
        "\n",
        "    for newspaper_name, config in NEWSPAPER_CONFIGS.items():\n",
        "        newspaper_url = config['url']\n",
        "        newspaper_language = config['language']\n",
        "\n",
        "            # 1. Get article links for the current newspaper\n",
        "        article_links = get_article_links(newspaper_url, newspaper_url)\n",
        "\n",
        "        if article_links:\n",
        "            # 2. Scrape article data for the current newspaper, with language and limit\n",
        "            newspaper_data = scrape_article_data(article_links, newspaper_language, limit=10)\n",
        "\n",
        "            # 3. Add 'Source' column to each article\n",
        "            for article in newspaper_data:\n",
        "                article['Source'] = newspaper_name\n",
        "\n",
        "            # 4. Accumulate all scraped data\n",
        "            all_scraped_data.extend(newspaper_data)\n",
        "        else:\n",
        "            print(f\"No article links found for {newspaper_name}.\")\n",
        "\n",
        "\n",
        "\n",
        "    # Save the combined results to a single CSV\n",
        "    save_to_csv(all_scraped_data, OUTPUT_ALL_HEADLINES_FILENAME)\n",
        "\n",
        "    # Load the combined data into a DataFrame\n",
        "    df_all_headlines = pd.read_csv(OUTPUT_ALL_HEADLINES_FILENAME)\n",
        "\n",
        "    # Display headlines in an HTML page with hyperlinks\n",
        "    current_date = datetime.now().strftime('%d-%m-%Y')\n",
        "\n",
        "    html_content_parts = [\n",
        "        f\"<h1>Latest News Headlines - {current_date}</h1>\\n\",\n",
        "        \"<ul>\\n\"\n",
        "    ]\n",
        "\n",
        "    for index, row in df_all_headlines.iterrows():\n",
        "        title = row['Title']\n",
        "        url = row['URL']\n",
        "        html_content_parts.append(f\"  <li><a href=\\\"{url}\\\" target=\\\"_blank\\\">{title}</a></li>\\n\")\n",
        "\n",
        "    html_content_parts.append(\"</ul>\")\n",
        "\n",
        "    new_html_content = \"\".join(html_content_parts)\n",
        "    display(HTML(new_html_content))\n",
        "\n",
        "    # New: Save the HTML content to a file\n",
        "    with open(HTML_OUTPUT_FILENAME, 'w', encoding='utf-8') as f:\n",
        "        f.write(new_html_content)"
      ]
    }
  ]
}